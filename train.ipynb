{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the path to the archive folder and the CSV file\n",
    "archive_folder = 'archive'\n",
    "csv_file_items = 'items.csv'\n",
    "csv_file_events = 'events1.csv'\n",
    "csv_file_users = 'users.csv'\n",
    "csv_file_items = os.path.join(archive_folder, csv_file_items)\n",
    "csv_file_events = os.path.join(archive_folder, csv_file_events)\n",
    "csv_file_users = os.path.join(archive_folder, csv_file_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count, avg, log1p, exp\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MerchandisingPricingModel\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Optional: Set log level to WARN to reduce verbosity\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items DataFrame Schema:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- variant: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- price_in_usd: integer (nullable = true)\n",
      "\n",
      "Events DataFrame Schema:\n",
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- ga_session_id: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- device: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      "\n",
      "Users DataFrame Schema:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- ltv: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Items DataFrame\n",
    "items_df = spark.read.csv(csv_file_items, header=True, inferSchema=True)\n",
    "print(\"Items DataFrame Schema:\")\n",
    "items_df.printSchema()\n",
    "\n",
    "# Load Events DataFrame\n",
    "events_df = spark.read.csv(csv_file_events, header=True, inferSchema=True)\n",
    "print(\"Events DataFrame Schema:\")\n",
    "events_df.printSchema()\n",
    "\n",
    "# Load Users DataFrame\n",
    "users_df = spark.read.csv(csv_file_users, header=True, inferSchema=True)\n",
    "print(\"Users DataFrame Schema:\")\n",
    "users_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events DataFrame Null Counts after dropping 'country' nulls:\n",
      "+-------+-------------+-------+------+----+-------+----+\n",
      "|user_id|ga_session_id|country|device|type|item_id|date|\n",
      "+-------+-------------+-------+------+----+-------+----+\n",
      "|      0|            0|      0|     0|   0|      0|   0|\n",
      "+-------+-------------+-------+------+----+-------+----+\n",
      "\n",
      "Items DataFrame Null Counts after dropping 'variant' nulls:\n",
      "+---+----+-----+-------+--------+------------+\n",
      "| id|name|brand|variant|category|price_in_usd|\n",
      "+---+----+-----+-------+--------+------------+\n",
      "|  0|   0|    0|      0|       0|           0|\n",
      "+---+----+-----+-------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# Drop rows with missing values in 'country' and 'variant' columns\n",
    "events_df = events_df.dropna(subset=['country'])\n",
    "items_df = items_df.dropna(subset=['variant'])\n",
    "\n",
    "# Verify that there are no nulls in the specified columns\n",
    "print(\"Events DataFrame Null Counts after dropping 'country' nulls:\")\n",
    "events_df.select([count(when(col(c).isNull(), c)).alias(c) for c in events_df.columns]).show()\n",
    "\n",
    "print(\"Items DataFrame Null Counts after dropping 'variant' nulls:\")\n",
    "items_df.select([count(when(col(c).isNull(), c)).alias(c) for c in items_df.columns]).show()\n",
    "\n",
    "# Convert date columns to DateType\n",
    "events_df = events_df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "users_df = users_df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- variant: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- price_in_usd: integer (nullable = true)\n",
      "\n",
      "+-------+-------+-------------+-------+-------+-----------+----------+---------------------------------+-------+------------------+---------+------------+\n",
      "|item_id|user_id|ga_session_id|country|device |type       |date      |name                             |brand  |variant           |category |price_in_usd|\n",
      "+-------+-------+-------------+-------+-------+-----------+----------+---------------------------------+-------+------------------+---------+------------+\n",
      "|94     |2133   |16909        |US     |mobile |purchase   |2020-11-01|Google Large Tote White          |Google |Single Option Only|Bags     |10          |\n",
      "|425    |2133   |16909        |US     |mobile |purchase   |2020-11-01|Google Heather Green Speckled Tee|Google | XL               |Apparel  |21          |\n",
      "|1      |5789   |16908        |SE     |desktop|purchase   |2020-11-01|Google KeepCup                   |Google |Single Option Only|New      |28          |\n",
      "|62     |5789   |16908        |SE     |desktop|purchase   |2020-11-01|Google Mini Kick Ball            |Google |Single Option Only|Fun      |2           |\n",
      "|842    |5808   |4267         |US     |mobile |add_to_cart|2020-11-01|NULL                             |NULL   |NULL              |NULL     |NULL        |\n",
      "|951    |5808   |4267         |US     |mobile |add_to_cart|2020-11-01|NULL                             |NULL   |NULL              |NULL     |NULL        |\n",
      "|950    |5808   |4267         |US     |mobile |add_to_cart|2020-11-01|NULL                             |NULL   |NULL              |NULL     |NULL        |\n",
      "|1068   |5808   |4267         |US     |mobile |add_to_cart|2020-11-01|NULL                             |NULL   |NULL              |NULL     |NULL        |\n",
      "|862    |5808   |4267         |US     |mobile |add_to_cart|2020-11-01|NULL                             |NULL   |NULL              |NULL     |NULL        |\n",
      "|1119   |5808   |4267         |US     |mobile |add_to_cart|2020-11-01|NULL                             |NULL   |NULL              |NULL     |NULL        |\n",
      "|252    |5808   |4267         |US     |mobile |add_to_cart|2020-11-01|Android Iconic Hat Black         |Android|Single Option Only|Clearance|9           |\n",
      "|1085   |5808   |4267         |US     |mobile |add_to_cart|2020-11-01|NULL                             |NULL   |NULL              |NULL     |NULL        |\n",
      "|1074   |5808   |4267         |US     |mobile |add_to_cart|2020-11-01|NULL                             |NULL   |NULL              |NULL     |NULL        |\n",
      "|1045   |5808   |4267         |US     |mobile |add_to_cart|2020-11-01|NULL                             |NULL   |NULL              |NULL     |NULL        |\n",
      "|953    |5808   |4267         |US     |mobile |add_to_cart|2020-11-01|NULL                             |NULL   |NULL              |NULL     |NULL        |\n",
      "|1058   |5808   |4267         |US     |mobile |add_to_cart|2020-11-01|NULL                             |NULL   |NULL              |NULL     |NULL        |\n",
      "|862    |5808   |4267         |US     |mobile |add_to_cart|2020-11-01|NULL                             |NULL   |NULL              |NULL     |NULL        |\n",
      "|1119   |5808   |4267         |US     |mobile |add_to_cart|2020-11-01|NULL                             |NULL   |NULL              |NULL     |NULL        |\n",
      "|1044   |5808   |4267         |US     |mobile |add_to_cart|2020-11-01|NULL                             |NULL   |NULL              |NULL     |NULL        |\n",
      "|950    |5808   |4267         |US     |mobile |add_to_cart|2020-11-01|NULL                             |NULL   |NULL              |NULL     |NULL        |\n",
      "+-------+-------+-------------+-------+-------+-----------+----------+---------------------------------+-------+------------------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Rename column 'id' to 'item_id' in items_df for consistency\n",
    "items_df = items_df.withColumnRenamed(\"id\", \"item_id\")\n",
    "\n",
    "# Print schema to confirm renaming\n",
    "items_df.printSchema()\n",
    "# Merge events_df with items_df to add item details to transactions\n",
    "all_transactions_name = events_df.join(\n",
    "    items_df.select(\"item_id\", \"name\", \"brand\", \"variant\", \"category\", \"price_in_usd\"),\n",
    "    on=\"item_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Preview the merged DataFrame\n",
    "all_transactions_name.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows: 754329\n",
      "Unique item_id Values: 1381\n"
     ]
    }
   ],
   "source": [
    "# Total number of rows in the DataFrame\n",
    "total_rows = all_transactions_name.count()\n",
    "print(f\"Total Rows: {total_rows}\")\n",
    "# Count of unique item_id values\n",
    "unique_item_ids = all_transactions_name.select(\"item_id\").distinct().count()\n",
    "print(f\"Unique item_id Values: {unique_item_ids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+-------+-------+-----------+----------+--------------------+------+------------------+--------+------------+\n",
      "|item_id|user_id|ga_session_id|country| device|       type|      date|                name| brand|           variant|category|price_in_usd|\n",
      "+-------+-------+-------------+-------+-------+-----------+----------+--------------------+------+------------------+--------+------------+\n",
      "|      0|   5115|        17001|     US| mobile|   purchase|2020-11-02|Google Land & Sea...|Google|Single Option Only| Apparel|          14|\n",
      "|      0|  20937|          217|     IN|desktop|add_to_cart|2020-11-18|Google Land & Sea...|Google|Single Option Only| Apparel|          14|\n",
      "|      0|  10904|        16401|     TR|desktop|   purchase|2020-11-03|Google Land & Sea...|Google|Single Option Only| Apparel|          14|\n",
      "|      0|  29457|        17113|     KR| mobile|   purchase|2020-11-05|Google Land & Sea...|Google|Single Option Only| Apparel|          14|\n",
      "|      0|  30148|        16175|     MT|desktop|   purchase|2020-11-05|Google Land & Sea...|Google|Single Option Only| Apparel|          14|\n",
      "|      0|  32087|        15869|     US|desktop|   purchase|2020-11-06|Google Land & Sea...|Google|Single Option Only| Apparel|          14|\n",
      "|      0|  18184|        16368|     ES|desktop|   purchase|2020-11-09|Google Land & Sea...|Google|Single Option Only| Apparel|          14|\n",
      "|      0|   2418|        15838|     US| mobile|   purchase|2020-11-16|Google Land & Sea...|Google|Single Option Only| Apparel|          14|\n",
      "|      0|   2406|         2748|     MX|desktop|add_to_cart|2020-11-17|Google Land & Sea...|Google|Single Option Only| Apparel|          14|\n",
      "|      0|   2406|         2748|     MX|desktop|add_to_cart|2020-11-17|Google Land & Sea...|Google|Single Option Only| Apparel|          14|\n",
      "|      0|   2406|         2748|     MX|desktop|add_to_cart|2020-11-17|Google Land & Sea...|Google|Single Option Only| Apparel|          14|\n",
      "|      0|  15260|         3723|     MX|desktop|add_to_cart|2020-11-17|Google Land & Sea...|Google|Single Option Only| Apparel|          14|\n",
      "|      0|  20983|         4172|     US|desktop|add_to_cart|2020-11-17|Google Land & Sea...|Google|Single Option Only| Apparel|          14|\n",
      "|      0|   6635|        16144|     IL| mobile|   purchase|2020-11-17|Google Land & Sea...|Google|Single Option Only| Apparel|          14|\n",
      "|      0| 242264|        11504|     US| mobile|add_to_cart|2020-11-17|Google Land & Sea...|Google|Single Option Only| Apparel|          14|\n",
      "|      0|  28249|         6706|     US| mobile|add_to_cart|2020-11-17|Google Land & Sea...|Google|Single Option Only| Apparel|          14|\n",
      "|      0|  11654|         4173|     US|desktop|add_to_cart|2020-11-17|Google Land & Sea...|Google|Single Option Only| Apparel|          14|\n",
      "|      0|  13857|          315|     US| mobile|add_to_cart|2020-11-17|Google Land & Sea...|Google|Single Option Only| Apparel|          14|\n",
      "|      0|  13829|         5350|     US|desktop|add_to_cart|2020-11-17|Google Land & Sea...|Google|Single Option Only| Apparel|          14|\n",
      "|      0|  11073|         2167|     US| mobile|add_to_cart|2020-11-18|Google Land & Sea...|Google|Single Option Only| Apparel|          14|\n",
      "+-------+-------+-------------+-------+-------+-----------+----------+--------------------+------+------------------+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, min\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Add a column with the smallest item_id for each name\n",
    "all_transactions_name = all_transactions_name.withColumn(\n",
    "    \"item_id_min\",\n",
    "    min(\"item_id\").over(Window.partitionBy(\"name\"))\n",
    ")\n",
    "# Replace item_id with item_id_min\n",
    "all_transactions_name = all_transactions_name.withColumn(\n",
    "    \"item_id\",\n",
    "    col(\"item_id_min\")\n",
    ")\n",
    "# Drop item_id_min column\n",
    "all_transactions_name = all_transactions_name.drop(\"item_id_min\")\n",
    "\n",
    "# Sort the DataFrame by item_id\n",
    "sorted_transactions = all_transactions_name.orderBy(\"item_id\", ascending=True)\n",
    "sorted_transactions.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+-------+-------+--------+----------+--------------------+------+------------------+--------------------+------------+\n",
      "|item_id|user_id|ga_session_id|country| device|    type|      date|                name| brand|           variant|            category|price_in_usd|\n",
      "+-------+-------+-------------+-------+-------+--------+----------+--------------------+------+------------------+--------------------+------------+\n",
      "|      0|   5115|        17001|     US| mobile|purchase|2020-11-02|Google Land & Sea...|Google|Single Option Only|             Apparel|          14|\n",
      "|      1|   5789|        16908|     SE|desktop|purchase|2020-11-01|      Google KeepCup|Google|Single Option Only|                 New|          28|\n",
      "|      2|  32033|        15864|     PT|desktop|purchase|2020-11-02|Google Land & Sea...|Google|Single Option Only|           Drinkware|          20|\n",
      "|      3|  11348|        17000|     US|desktop|purchase|2020-11-02|Google Unisex Eco...|Google|                XL| Uncategorized Items|          18|\n",
      "|      4|   1858|        15945|     HK|desktop|purchase|2020-11-09|Google Chicago Ca...|Google|Single Option Only|   Campus Collection|          11|\n",
      "|      5|  20885|        17626|     US|desktop|purchase|2020-11-05|Google SF Campus ...|Google|                SM|           Clearance|          32|\n",
      "|      6|  20146|        16392|     US|desktop|purchase|2020-11-01|Google Chicago Ca...|Google|                LG|   Campus Collection|          14|\n",
      "|      7|   5115|        17001|     US| mobile|purchase|2020-11-02|Google SF Campus ...|Google|                MD|           Clearance|          14|\n",
      "|      8|   2130|        16907|     PE| mobile|purchase|2020-11-01|Super G Unisex Jo...|Google|                MD|       Shop by Brand|          30|\n",
      "|      9|  12719|        17556|     NL|desktop|purchase|2020-11-06|Mommy Works at Go...|Google|Single Option Only|         Small Goods|          16|\n",
      "|     10|  23497|        17408|     US|desktop|purchase|2020-11-09|BLM Unisex Pullov...|Google|                MD|  Black Lives Matter|          46|\n",
      "|     11|  15819|        15988|     GR| mobile|purchase|2020-11-03|Google See-No Hea...|Google|Single Option Only|Electronics Acces...|          10|\n",
      "|     12|   2130|        16907|     PE| mobile|purchase|2020-11-01|Google Sherpa Zip...|Google|                MD|             Apparel|          55|\n",
      "|     14|  17185|        15813|     CA| mobile|purchase|2020-11-04|Google Campus Bik...|Google|Single Option Only|           Lifestyle|          22|\n",
      "|     16|  27711|        16130|     US| mobile|purchase|2020-11-06|Google Crewneck S...|Google|                LG|             Apparel|          44|\n",
      "|     17|   6138|        14876|     CA| mobile|purchase|2020-11-04|Google Mural Stic...|Google|Single Option Only|         Small Goods|           3|\n",
      "|     18|  21710|        16404|     MA| mobile|purchase|2020-11-03|Google F/C Longsl...|Google|                XS|             Apparel|          24|\n",
      "|     19|  20451|        10776|     US|desktop|purchase|2020-11-02|Google Speckled B...|Google|Single Option Only|                 New|          16|\n",
      "|     20|  31679|        15182|     US|desktop|purchase|2020-11-06|Google NYC Campus...|Google|Single Option Only|   Campus Collection|           1|\n",
      "|     21|  11075|        17553|     US| mobile|purchase|2020-11-06|Google Campus Bik...|Google|Single Option Only|                Bags|           9|\n",
      "+-------+-------+-------------+-------+-------+--------+----------+--------------------+------+------------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicates based on item_id\n",
    "unique_items = sorted_transactions.dropDuplicates([\"item_id\"])\n",
    "unique_items.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------------------------+-----------------------+\n",
      "|item_id|name                                  |category               |\n",
      "+-------+--------------------------------------+-----------------------+\n",
      "|0      |Google Land & Sea Cotton Cap          |Apparel                |\n",
      "|1      |Google KeepCup                        |New                    |\n",
      "|2      |Google Land & Sea Nalgene Water Bottle|Drinkware              |\n",
      "|3      |Google Unisex Eco Tee Black           |Uncategorized Items    |\n",
      "|4      |Google Chicago Campus Bottle          |Campus Collection      |\n",
      "|5      |Google SF Campus Zip Hoodie           |Clearance              |\n",
      "|6      |Google Chicago Campus Unisex Tee      |Campus Collection      |\n",
      "|7      |Google SF Campus Unisex Tee           |Clearance              |\n",
      "|8      |Super G Unisex Joggers                |Shop by Brand          |\n",
      "|9      |Mommy Works at Google Book            |Small Goods            |\n",
      "|10     |BLM Unisex Pullover Hoodie            |Black Lives Matter     |\n",
      "|11     |Google See-No Hear-No Set             |Electronics Accessories|\n",
      "|12     |Google Sherpa Zip Hoodie Navy         |Apparel                |\n",
      "|14     |Google Campus Bike Bottle             |Lifestyle              |\n",
      "|16     |Google Crewneck Sweatshirt Green      |Apparel                |\n",
      "|17     |Google Mural Sticker Sheet            |Small Goods            |\n",
      "|18     |Google F/C Longsleeve Charcoal        |Apparel                |\n",
      "|19     |Google Speckled Beanie Navy           |New                    |\n",
      "|20     |Google NYC Campus Sticker             |Campus Collection      |\n",
      "|21     |Google Campus Bike Tote Navy          |Bags                   |\n",
      "+-------+--------------------------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename columns to match the desired schema\n",
    "transactions_new = unique_items.select(\n",
    "    col(\"item_id\"),\n",
    "    col(\"name\"),\n",
    "    col(\"category\")\n",
    ")\n",
    "\n",
    "# Show the new DataFrame\n",
    "transactions_new.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   item_id                                    name             category\n",
      "0        0            Google Land & Sea Cotton Cap              Apparel\n",
      "1        1                          Google KeepCup                  New\n",
      "2        2  Google Land & Sea Nalgene Water Bottle            Drinkware\n",
      "3        3             Google Unisex Eco Tee Black  Uncategorized Items\n",
      "4        4            Google Chicago Campus Bottle    Campus Collection\n"
     ]
    }
   ],
   "source": [
    "# Convert the Spark DataFrame to Pandas for further processing\n",
    "data = transactions_new.toPandas()\n",
    "\n",
    "# Verify the conversion\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "Apparel                    109\n",
       "Campus Collection           66\n",
       "New                         43\n",
       "Accessories                 41\n",
       "Clearance                   28\n",
       "Bags                        23\n",
       "Office                      17\n",
       "Shop by Brand               16\n",
       "Lifestyle                   13\n",
       "Uncategorized Items         12\n",
       "Drinkware                   12\n",
       "Stationery                   8\n",
       "Small Goods                  6\n",
       "Writing Instruments          5\n",
       "Google                       4\n",
       "Gift Cards                   4\n",
       "Notebooks & Journals         2\n",
       "Electronics Accessories      1\n",
       "Black Lives Matter           1\n",
       "Fun                          1\n",
       "Eco-Friendly                 1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Replace null or None values with \"Unknown\" in the category column\n",
    "transactions_new = transactions_new.withColumn(\n",
    "    \"category\",\n",
    "    when(col(\"category\").isNull(), \"Uncategorized Items\").otherwise(col(\"category\"))\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+--------------------------+--------------------------------------+\n",
      "|category               |tokens                    |name                                  |\n",
      "+-----------------------+--------------------------+--------------------------------------+\n",
      "|Apparel                |[apparel]                 |Google Land & Sea Cotton Cap          |\n",
      "|New                    |[new]                     |Google KeepCup                        |\n",
      "|Drinkware              |[drinkware]               |Google Land & Sea Nalgene Water Bottle|\n",
      "|Uncategorized Items    |[uncategorized, items]    |Google Unisex Eco Tee Black           |\n",
      "|Campus Collection      |[campus, collection]      |Google Chicago Campus Bottle          |\n",
      "|Clearance              |[clearance]               |Google SF Campus Zip Hoodie           |\n",
      "|Campus Collection      |[campus, collection]      |Google Chicago Campus Unisex Tee      |\n",
      "|Clearance              |[clearance]               |Google SF Campus Unisex Tee           |\n",
      "|Shop by Brand          |[shop, by, brand]         |Super G Unisex Joggers                |\n",
      "|Small Goods            |[small, goods]            |Mommy Works at Google Book            |\n",
      "|Black Lives Matter     |[black, lives, matter]    |BLM Unisex Pullover Hoodie            |\n",
      "|Electronics Accessories|[electronics, accessories]|Google See-No Hear-No Set             |\n",
      "|Apparel                |[apparel]                 |Google Sherpa Zip Hoodie Navy         |\n",
      "|Lifestyle              |[lifestyle]               |Google Campus Bike Bottle             |\n",
      "|Apparel                |[apparel]                 |Google Crewneck Sweatshirt Green      |\n",
      "|Small Goods            |[small, goods]            |Google Mural Sticker Sheet            |\n",
      "|Apparel                |[apparel]                 |Google F/C Longsleeve Charcoal        |\n",
      "|New                    |[new]                     |Google Speckled Beanie Navy           |\n",
      "|Campus Collection      |[campus, collection]      |Google NYC Campus Sticker             |\n",
      "|Bags                   |[bags]                    |Google Campus Bike Tote Navy          |\n",
      "+-----------------------+--------------------------+--------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# Tokenize the category column\n",
    "tokenizer = Tokenizer(inputCol=\"category\", outputCol=\"tokens\")\n",
    "tokenized_df = tokenizer.transform(transactions_new)\n",
    "\n",
    "# Show tokenized data\n",
    "tokenized_df.select(\"category\", \"tokens\",\"name\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------------------------------------------------------------------+\n",
      "|category               |features                                                               |\n",
      "+-----------------------+-----------------------------------------------------------------------+\n",
      "|Apparel                |(32,[0],[1.3277981544382822])                                          |\n",
      "|New                    |(32,[3],[2.2440888863124373])                                          |\n",
      "|Drinkware              |(32,[14],[3.4633291627691616])                                         |\n",
      "|Uncategorized Items    |(32,[11,12],[3.3892211906154395,3.3892211906154395])                   |\n",
      "|Campus Collection      |(32,[1,2],[1.8235859008397322,1.8235859008397322])                     |\n",
      "|Clearance              |(32,[5],[2.660982690244224])                                           |\n",
      "|Campus Collection      |(32,[1,2],[1.8235859008397322,1.8235859008397322])                     |\n",
      "|Clearance              |(32,[5],[2.660982690244224])                                           |\n",
      "|Shop by Brand          |(32,[8,9,10],[3.195065176174482,3.195065176174482,3.195065176174482])  |\n",
      "|Small Goods            |(32,[16,17],[4.082368371175385,4.082368371175385])                     |\n",
      "|Black Lives Matter     |(32,[27,30,31],[5.335131339670753,5.335131339670753,5.335131339670753])|\n",
      "|Electronics Accessories|(32,[4,29],[2.267078404537136,5.335131339670753])                      |\n",
      "|Apparel                |(32,[0],[1.3277981544382822])                                          |\n",
      "|Lifestyle              |(32,[13],[3.3892211906154395])                                         |\n",
      "|Apparel                |(32,[0],[1.3277981544382822])                                          |\n",
      "|Small Goods            |(32,[16,17],[4.082368371175385,4.082368371175385])                     |\n",
      "|Apparel                |(32,[0],[1.3277981544382822])                                          |\n",
      "|New                    |(32,[3],[2.2440888863124373])                                          |\n",
      "|Campus Collection      |(32,[1,2],[1.8235859008397322,1.8235859008397322])                     |\n",
      "|Bags                   |(32,[6],[2.8502246898827526])                                          |\n",
      "+-----------------------+-----------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "\n",
    "# Step 3.1: Compute term frequency (TF)\n",
    "count_vectorizer = CountVectorizer(inputCol=\"tokens\", outputCol=\"raw_features\")\n",
    "count_model = count_vectorizer.fit(tokenized_df)\n",
    "tf_df = count_model.transform(tokenized_df)\n",
    "\n",
    "# Step 3.2: Compute inverse document frequency (IDF)\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idf_model = idf.fit(tf_df)\n",
    "tfidf_df = idf_model.transform(tf_df)\n",
    "\n",
    "# Show TF-IDF results\n",
    "tfidf_df.select(\"category\", \"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# Convert 'features' column from Vector to array (dense representation)\n",
    "dense_df = tfidf_df.withColumn(\"features_dense\", vector_to_array(\"features\"))\n",
    "\n",
    "# Show the dense features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+-----------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|name                                  |category               |features_dense                                                                                                                                                                                            |\n",
      "+--------------------------------------+-----------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Google Land & Sea Cotton Cap          |Apparel                |[1.3277981544382822, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]                           |\n",
      "|Google KeepCup                        |New                    |[0.0, 0.0, 0.0, 2.2440888863124373, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]                           |\n",
      "|Google Land & Sea Nalgene Water Bottle|Drinkware              |[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.4633291627691616, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]                           |\n",
      "|Google Unisex Eco Tee Black           |Uncategorized Items    |[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.3892211906154395, 3.3892211906154395, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]            |\n",
      "|Google Chicago Campus Bottle          |Campus Collection      |[0.0, 1.8235859008397322, 1.8235859008397322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]            |\n",
      "|Google SF Campus Zip Hoodie           |Clearance              |[0.0, 0.0, 0.0, 0.0, 0.0, 2.660982690244224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]                            |\n",
      "|Google Chicago Campus Unisex Tee      |Campus Collection      |[0.0, 1.8235859008397322, 1.8235859008397322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]            |\n",
      "|Google SF Campus Unisex Tee           |Clearance              |[0.0, 0.0, 0.0, 0.0, 0.0, 2.660982690244224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]                            |\n",
      "|Super G Unisex Joggers                |Shop by Brand          |[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.195065176174482, 3.195065176174482, 3.195065176174482, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]|\n",
      "|Mommy Works at Google Book            |Small Goods            |[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.082368371175385, 4.082368371175385, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]              |\n",
      "|BLM Unisex Pullover Hoodie            |Black Lives Matter     |[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.335131339670753, 0.0, 0.0, 5.335131339670753, 5.335131339670753]|\n",
      "|Google See-No Hear-No Set             |Electronics Accessories|[0.0, 0.0, 0.0, 0.0, 2.267078404537136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.335131339670753, 0.0, 0.0]              |\n",
      "|Google Sherpa Zip Hoodie Navy         |Apparel                |[1.3277981544382822, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]                           |\n",
      "|Google Campus Bike Bottle             |Lifestyle              |[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.3892211906154395, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]                           |\n",
      "|Google Crewneck Sweatshirt Green      |Apparel                |[1.3277981544382822, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]                           |\n",
      "|Google Mural Sticker Sheet            |Small Goods            |[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.082368371175385, 4.082368371175385, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]              |\n",
      "|Google F/C Longsleeve Charcoal        |Apparel                |[1.3277981544382822, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]                           |\n",
      "|Google Speckled Beanie Navy           |New                    |[0.0, 0.0, 0.0, 2.2440888863124373, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]                           |\n",
      "|Google NYC Campus Sticker             |Campus Collection      |[0.0, 1.8235859008397322, 1.8235859008397322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]            |\n",
      "|Google Campus Bike Tote Navy          |Bags                   |[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.8502246898827526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]                           |\n",
      "+--------------------------------------+-----------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dense_df.select(\"name\",\"category\", \"features_dense\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('item_id', IntegerType(), True), StructField('name', StringType(), True), StructField('category', StringType(), True), StructField('tokens', ArrayType(StringType(), True), True), StructField('raw_features', VectorUDT(), True), StructField('features', VectorUDT(), True), StructField('features_dense', ArrayType(DoubleType(), False), False)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+--------------------------------------+-----------------+\n",
      "|name1                       |name2                                 |cosine_similarity|\n",
      "+----------------------------+--------------------------------------+-----------------+\n",
      "|Google Land & Sea Cotton Cap|Google KeepCup                        |0.0              |\n",
      "|Google Land & Sea Cotton Cap|Google Land & Sea Nalgene Water Bottle|0.0              |\n",
      "|Google Land & Sea Cotton Cap|Google Unisex Eco Tee Black           |0.0              |\n",
      "|Google Land & Sea Cotton Cap|Google Chicago Campus Bottle          |0.0              |\n",
      "|Google Land & Sea Cotton Cap|Google SF Campus Zip Hoodie           |0.0              |\n",
      "|Google Land & Sea Cotton Cap|Google Chicago Campus Unisex Tee      |0.0              |\n",
      "|Google Land & Sea Cotton Cap|Google SF Campus Unisex Tee           |0.0              |\n",
      "|Google Land & Sea Cotton Cap|Super G Unisex Joggers                |0.0              |\n",
      "|Google Land & Sea Cotton Cap|Mommy Works at Google Book            |0.0              |\n",
      "|Google Land & Sea Cotton Cap|BLM Unisex Pullover Hoodie            |0.0              |\n",
      "|Google Land & Sea Cotton Cap|Google See-No Hear-No Set             |0.0              |\n",
      "|Google Land & Sea Cotton Cap|Google Sherpa Zip Hoodie Navy         |1.0              |\n",
      "|Google Land & Sea Cotton Cap|Google Campus Bike Bottle             |0.0              |\n",
      "|Google Land & Sea Cotton Cap|Google Crewneck Sweatshirt Green      |1.0              |\n",
      "|Google Land & Sea Cotton Cap|Google Mural Sticker Sheet            |0.0              |\n",
      "|Google Land & Sea Cotton Cap|Google F/C Longsleeve Charcoal        |1.0              |\n",
      "|Google Land & Sea Cotton Cap|Google Speckled Beanie Navy           |0.0              |\n",
      "|Google Land & Sea Cotton Cap|Google NYC Campus Sticker             |0.0              |\n",
      "|Google Land & Sea Cotton Cap|Google Campus Bike Tote Navy          |0.0              |\n",
      "|Google Land & Sea Cotton Cap|Google Infant Charcoal Onesie         |1.0              |\n",
      "+----------------------------+--------------------------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "# Select name and features_dense for self-join\n",
    "df1 = dense_df.select(\n",
    "    col(\"name\").alias(\"name1\"),\n",
    "    col(\"features_dense\").alias(\"features1\")\n",
    ")\n",
    "df2 = dense_df.select(\n",
    "    col(\"name\").alias(\"name2\"),\n",
    "    col(\"features_dense\").alias(\"features2\")\n",
    ")\n",
    "\n",
    "# Perform cross join\n",
    "cross_df = df1.crossJoin(df2)\n",
    "\n",
    "# Compute dot product using aggregate and transform with proper casting\n",
    "dot_product_expr = \"\"\"\n",
    "    aggregate(\n",
    "        transform(features1, (x, i) -> x * features2[i]),\n",
    "        cast(0.0 as double),\n",
    "        (acc, x) -> acc + x\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "cross_df = cross_df.withColumn(\"dot_product\", expr(dot_product_expr))\n",
    "\n",
    "# Compute norms of feature vectors with proper casting\n",
    "norm1_expr = \"\"\"\n",
    "    sqrt(\n",
    "        aggregate(\n",
    "            features1,\n",
    "            cast(0.0 as double),\n",
    "            (acc, x) -> acc + x * x\n",
    "        )\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "norm2_expr = \"\"\"\n",
    "    sqrt(\n",
    "        aggregate(\n",
    "            features2,\n",
    "            cast(0.0 as double),\n",
    "            (acc, x) -> acc + x * x\n",
    "        )\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "cross_df = cross_df.withColumn(\"norm1\", expr(norm1_expr)) \\\n",
    "                   .withColumn(\"norm2\", expr(norm2_expr))\n",
    "\n",
    "# Compute cosine similarity and handle division by zero\n",
    "cosine_similarity_expr = \"\"\"\n",
    "    CASE \n",
    "        WHEN norm1 * norm2 = 0 THEN 0 \n",
    "        ELSE dot_product / (norm1 * norm2) \n",
    "    END\n",
    "\"\"\"\n",
    "\n",
    "cross_df = cross_df.withColumn(\"cosine_similarity\", expr(cosine_similarity_expr))\n",
    "\n",
    "# Filter out self-similarity\n",
    "similarity_df = cross_df.filter(col(\"name1\") != col(\"name2\"))\n",
    "\n",
    "# Select relevant columns\n",
    "similarity_df = similarity_df.select(\"name1\", \"name2\", \"cosine_similarity\")\n",
    "\n",
    "# Show the similarity matrix\n",
    "similarity_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------------+--------+-----------------+\n",
      "|item_id|recommended_item               |category|cosine_similarity|\n",
      "+-------+-------------------------------+--------+-----------------+\n",
      "|21     |Google Campus Bike Tote Navy   |Bags    |1.0              |\n",
      "|103    |Google Incognito Zip Pack      |Bags    |1.0              |\n",
      "|333    |Google Incognito Dopp Kit V2   |Bags    |1.0              |\n",
      "|297    |Google Striped Penny Pouch     |Bags    |1.0              |\n",
      "|315    |Google Utility Bag Grey        |Bags    |1.0              |\n",
      "|58     |Google Utility BackPack        |Bags    |1.0              |\n",
      "|299    |Google Packable Bag Black      |Bags    |1.0              |\n",
      "|59     |Google Incognito Techpack V2   |Bags    |1.0              |\n",
      "|365    |Google Confetti Accessory Pouch|Bags    |1.0              |\n",
      "|94     |Google Large Tote White        |Bags    |1.0              |\n",
      "+-------+-------------------------------+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "def query_cosine_matrix(item_name, similarity_df, items_df, k=10):\n",
    "    \"\"\"\n",
    "    Query the cosine similarity matrix for top-k similar items.\n",
    "\n",
    "    Parameters:\n",
    "    ---\n",
    "    item_name : str\n",
    "        The name of the item to query.\n",
    "    similarity_df : pyspark.sql.DataFrame\n",
    "        DataFrame containing cosine similarity with ('name1', 'name2', 'cosine_similarity').\n",
    "    items_df : pyspark.sql.DataFrame\n",
    "        DataFrame containing item details with ('item_id', 'name', 'category').\n",
    "    k : int\n",
    "        The number of top similar items to return.\n",
    "\n",
    "    Returns:\n",
    "    ---\n",
    "    pyspark.sql.DataFrame:\n",
    "        DataFrame with top-k similar items and their details.\n",
    "    \"\"\"\n",
    "    # Alias the DataFrames to avoid ambiguity\n",
    "    similarity_alias = similarity_df.alias(\"similarity\")\n",
    "    items_alias = items_df.alias(\"items\")\n",
    "\n",
    "    # Filter the similarity_df for the given item\n",
    "    filtered_df = similarity_alias.filter(col(\"similarity.name1\") == item_name)\n",
    "\n",
    "    # Get the top-k similar items\n",
    "    top_k = filtered_df.orderBy(col(\"similarity.cosine_similarity\").desc()).limit(k)\n",
    "\n",
    "    # Join with items_df to get item details\n",
    "    result = top_k.join(\n",
    "        items_alias,\n",
    "        col(\"similarity.name2\") == col(\"items.name\"),\n",
    "        how='left'\n",
    "    ).select(\n",
    "        col(\"items.item_id\"),\n",
    "        col(\"items.name\").alias(\"recommended_item\"),\n",
    "        col(\"items.category\"),\n",
    "        col(\"similarity.cosine_similarity\")\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "# Example Usage\n",
    "input_item = \"Google Flat Front Bag Grey\"\n",
    "top_similar_items = query_cosine_matrix(input_item, similarity_df, transactions_new, k=10)\n",
    "\n",
    "# Show the top-k similar items\n",
    "top_similar_items.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+-------+------+-----------+----------+----+-----+-------+--------+------------+\n",
      "|item_id|user_id|ga_session_id|country|device|type       |date      |name|brand|variant|category|price_in_usd|\n",
      "+-------+-------+-------------+-------+------+-----------+----------+----+-----+-------+--------+------------+\n",
      "|835    |5808   |4267         |US     |mobile|add_to_cart|2020-11-01|NULL|NULL |NULL   |NULL    |NULL        |\n",
      "|835    |5808   |4267         |US     |mobile|add_to_cart|2020-11-01|NULL|NULL |NULL   |NULL    |NULL        |\n",
      "|835    |5808   |4267         |US     |mobile|add_to_cart|2020-11-01|NULL|NULL |NULL   |NULL    |NULL        |\n",
      "|835    |5808   |4267         |US     |mobile|add_to_cart|2020-11-01|NULL|NULL |NULL   |NULL    |NULL        |\n",
      "|835    |5808   |4267         |US     |mobile|add_to_cart|2020-11-01|NULL|NULL |NULL   |NULL    |NULL        |\n",
      "|835    |5808   |4267         |US     |mobile|add_to_cart|2020-11-01|NULL|NULL |NULL   |NULL    |NULL        |\n",
      "|835    |5808   |4267         |US     |mobile|add_to_cart|2020-11-01|NULL|NULL |NULL   |NULL    |NULL        |\n",
      "|835    |5808   |4267         |US     |mobile|add_to_cart|2020-11-01|NULL|NULL |NULL   |NULL    |NULL        |\n",
      "|835    |5808   |4267         |US     |mobile|add_to_cart|2020-11-01|NULL|NULL |NULL   |NULL    |NULL        |\n",
      "|835    |5808   |4267         |US     |mobile|add_to_cart|2020-11-01|NULL|NULL |NULL   |NULL    |NULL        |\n",
      "|835    |5808   |4267         |US     |mobile|add_to_cart|2020-11-01|NULL|NULL |NULL   |NULL    |NULL        |\n",
      "|835    |5808   |4267         |US     |mobile|add_to_cart|2020-11-01|NULL|NULL |NULL   |NULL    |NULL        |\n",
      "|835    |5808   |4267         |US     |mobile|add_to_cart|2020-11-01|NULL|NULL |NULL   |NULL    |NULL        |\n",
      "|835    |5808   |4267         |US     |mobile|add_to_cart|2020-11-01|NULL|NULL |NULL   |NULL    |NULL        |\n",
      "|835    |5808   |4267         |US     |mobile|add_to_cart|2020-11-01|NULL|NULL |NULL   |NULL    |NULL        |\n",
      "|835    |5808   |4267         |US     |mobile|add_to_cart|2020-11-01|NULL|NULL |NULL   |NULL    |NULL        |\n",
      "|835    |5808   |4267         |US     |mobile|add_to_cart|2020-11-01|NULL|NULL |NULL   |NULL    |NULL        |\n",
      "|835    |5808   |4267         |US     |mobile|add_to_cart|2020-11-01|NULL|NULL |NULL   |NULL    |NULL        |\n",
      "|835    |5808   |4267         |US     |mobile|add_to_cart|2020-11-01|NULL|NULL |NULL   |NULL    |NULL        |\n",
      "|835    |5808   |4267         |US     |mobile|add_to_cart|2020-11-01|NULL|NULL |NULL   |NULL    |NULL        |\n",
      "+-------+-------+-------------+-------+------+-----------+----------+----+-----+-------+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_transactions_name.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|type       |event_type|\n",
      "+-----------+----------+\n",
      "|purchase   |3         |\n",
      "|purchase   |3         |\n",
      "|purchase   |3         |\n",
      "|purchase   |3         |\n",
      "|add_to_cart|1         |\n",
      "|add_to_cart|1         |\n",
      "|add_to_cart|1         |\n",
      "|add_to_cart|1         |\n",
      "|add_to_cart|1         |\n",
      "|add_to_cart|1         |\n",
      "+-----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Define the event type mapping\n",
    "event_type_mapping = {\n",
    "    'add_to_cart': 1,\n",
    "    'begin_checkout': 2,\n",
    "    'purchase': 3\n",
    "}\n",
    "\n",
    "# Apply the mapping to create a new 'event_type' column\n",
    "all_transactions_name = all_transactions_name.withColumn(\n",
    "    \"event_type\",\n",
    "    when(col(\"type\") == \"add_to_cart\", 1)\n",
    "    .when(col(\"type\") == \"begin_checkout\", 2)\n",
    "    .when(col(\"type\") == \"purchase\", 3)\n",
    "    .otherwise(0)  # Assign 0 or another value for unknown types\n",
    ")\n",
    "\n",
    "# Verify the mapping\n",
    "all_transactions_name.select(\"type\", \"event_type\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\n",
      "|user_id|user_id_encoded|\n",
      "+-------+---------------+\n",
      "|2133   |13829.0        |\n",
      "|2133   |13829.0        |\n",
      "|5789   |14023.0        |\n",
      "|5789   |14023.0        |\n",
      "|5808   |4188.0         |\n",
      "|5808   |4188.0         |\n",
      "|5808   |4188.0         |\n",
      "|5808   |4188.0         |\n",
      "|5808   |4188.0         |\n",
      "|5808   |4188.0         |\n",
      "+-------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Initialize StringIndexer for 'user_id'\n",
    "user_indexer = StringIndexer(inputCol=\"user_id\", outputCol=\"user_id_encoded\")\n",
    "\n",
    "# Fit the indexer on the DataFrame\n",
    "user_indexer_model = user_indexer.fit(all_transactions_name)\n",
    "\n",
    "# Transform the DataFrame to include the encoded 'user_id'\n",
    "all_transactions_name = user_indexer_model.transform(all_transactions_name)\n",
    "\n",
    "# Verify the encoding\n",
    "all_transactions_name.select(\"user_id\", \"user_id_encoded\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\n",
      "|item_id|item_id_encoded|\n",
      "+-------+---------------+\n",
      "|835    |0.0            |\n",
      "|835    |0.0            |\n",
      "|835    |0.0            |\n",
      "|835    |0.0            |\n",
      "|835    |0.0            |\n",
      "|835    |0.0            |\n",
      "|835    |0.0            |\n",
      "|835    |0.0            |\n",
      "|835    |0.0            |\n",
      "|835    |0.0            |\n",
      "+-------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Initialize and fit the StringIndexer for item_id\n",
    "item_indexer = StringIndexer(inputCol=\"item_id\", outputCol=\"item_id_encoded\")\n",
    "item_indexer_model = item_indexer.fit(all_transactions_name)\n",
    "\n",
    "# Transform the dataset to include item_id_encoded\n",
    "all_transactions_name = item_indexer_model.transform(all_transactions_name)\n",
    "\n",
    "# Show the transformed dataset\n",
    "all_transactions_name.select(\"item_id\", \"item_id_encoded\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+\n",
      "|user_id|item_id|event_type|\n",
      "+-------+-------+----------+\n",
      "|4188.0 |0.0    |1         |\n",
      "|4188.0 |0.0    |1         |\n",
      "|4188.0 |0.0    |1         |\n",
      "|4188.0 |0.0    |1         |\n",
      "|4188.0 |0.0    |1         |\n",
      "|4188.0 |0.0    |1         |\n",
      "|4188.0 |0.0    |1         |\n",
      "|4188.0 |0.0    |1         |\n",
      "|4188.0 |0.0    |1         |\n",
      "|4188.0 |0.0    |1         |\n",
      "+-------+-------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the necessary columns\n",
    "final_df = all_transactions_name.select(\n",
    "    col(\"user_id_encoded\").alias(\"user_id\"),\n",
    "    col(\"item_id_encoded\").alias(\"item_id\"),\n",
    "    col(\"event_type\")\n",
    ")\n",
    "\n",
    "# Show the final DataFrame\n",
    "final_df.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Extract labels from the StringIndexer model\n",
    "user_labels = user_indexer_model.labels\n",
    "\n",
    "# Create mapping dictionaries\n",
    "user_to_user_encoded = {label: idx for idx, label in enumerate(user_labels)}\n",
    "user_encoded_to_user = {idx: label for idx, label in enumerate(user_labels)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|event_type| count|\n",
      "+----------+------+\n",
      "|         1|663305|\n",
      "|         3| 15475|\n",
      "|         2| 75549|\n",
      "+----------+------+\n",
      "\n",
      "+-------+-------+----------+\n",
      "|user_id|item_id|event_type|\n",
      "+-------+-------+----------+\n",
      "|4188.0 |0.0    |1         |\n",
      "|4188.0 |0.0    |1         |\n",
      "|4188.0 |0.0    |1         |\n",
      "|4188.0 |0.0    |1         |\n",
      "|4188.0 |0.0    |1         |\n",
      "|4188.0 |0.0    |1         |\n",
      "|4188.0 |0.0    |1         |\n",
      "|4188.0 |0.0    |1         |\n",
      "|4188.0 |0.0    |1         |\n",
      "|4188.0 |0.0    |1         |\n",
      "+-------+-------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the counts of each event_type\n",
    "final_df.groupBy(\"event_type\").count().show()\n",
    "\n",
    "# Display sample data\n",
    "final_df.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+\n",
      "|user_id|item_id|event_type|\n",
      "+-------+-------+----------+\n",
      "|4188.0 |0.0    |1         |\n",
      "|4188.0 |0.0    |1         |\n",
      "|4188.0 |0.0    |1         |\n",
      "|4188.0 |0.0    |1         |\n",
      "|4188.0 |0.0    |1         |\n",
      "|4188.0 |0.0    |1         |\n",
      "|4188.0 |0.0    |1         |\n",
      "|4188.0 |0.0    |1         |\n",
      "|4188.0 |0.0    |1         |\n",
      "|4188.0 |0.0    |1         |\n",
      "+-------+-------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Count: 603709\n",
      "Validation Data Count: 150620\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training (80%) and validation (20%) sets\n",
    "train_df, val_df = final_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Training Data Count: {train_df.count()}\")\n",
    "print(f\"Validation Data Count: {val_df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS model training completed.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "# Initialize the ALS model\n",
    "als = ALS(\n",
    "    maxIter=10,\n",
    "    regParam=0.1,\n",
    "    userCol=\"user_id\",\n",
    "    itemCol=\"item_id\",\n",
    "    ratingCol=\"event_type\",\n",
    "    coldStartStrategy=\"drop\",  # To handle NaN predictions\n",
    "    nonnegative=True\n",
    ")\n",
    "\n",
    "# Train the ALS model on the training data\n",
    "als_model = als.fit(train_df)\n",
    "\n",
    "print(\"ALS model training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on validation data: 0.2222\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Make predictions on the validation set\n",
    "predictions = als_model.transform(val_df)\n",
    "\n",
    "# Initialize the evaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"event_type\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on validation data: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|user_id|recommendations                                                                                                                                                                    |\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|31     |[{404, 2.1045675}, {334, 2.0156727}, {296, 1.9900073}, {300, 1.9811398}, {360, 1.9764503}, {384, 1.9761128}, {283, 1.9755534}, {353, 1.974976}, {245, 1.9745234}, {374, 1.9645573}]|\n",
      "|34     |[{404, 1.8041916}, {360, 1.7999492}, {334, 1.7909286}, {375, 1.7745564}, {353, 1.7631757}, {391, 1.7532632}, {384, 1.7379528}, {296, 1.737398}, {245, 1.733621}, {283, 1.729852}]  |\n",
      "|53     |[{404, 1.9829658}, {334, 1.9108629}, {360, 1.8817106}, {296, 1.880996}, {353, 1.8737153}, {300, 1.8730516}, {384, 1.8702929}, {283, 1.8683498}, {245, 1.8680785}, {391, 1.8564993}]|\n",
      "|65     |[{404, 2.071029}, {334, 1.9866619}, {296, 1.9599723}, {300, 1.9511704}, {360, 1.9499937}, {384, 1.946864}, {353, 1.9466778}, {283, 1.945919}, {245, 1.9449538}, {374, 1.9343408}]  |\n",
      "|78     |[{404, 1.9068747}, {334, 1.8692049}, {360, 1.8625321}, {353, 1.837073}, {375, 1.8268809}, {296, 1.8244936}, {391, 1.8239326}, {384, 1.8203744}, {245, 1.8168187}, {300, 1.8162459}]|\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate top 10 item recommendations for each user\n",
    "user_recommendations = als_model.recommendForAllUsers(10)\n",
    "\n",
    "# Show recommendations for a few users\n",
    "user_recommendations.select(\"user_id\", \"recommendations\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model RMSE: 0.2000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Define a parameter grid\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(als.rank, [10, 20, 30]) \\\n",
    "    .addGrid(als.regParam, [0.05, 0.1, 0.15]) \\\n",
    "    .build()\n",
    "\n",
    "# Define the evaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"event_type\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "# Initialize CrossValidator\n",
    "crossval = CrossValidator(\n",
    "    estimator=als,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=5\n",
    ")\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cv_model = crossval.fit(train_df)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_model = cv_model.bestModel\n",
    "predictions = best_model.transform(val_df)\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Best Model RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|user_id|recommendations                                                                                                                                                                    |\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|31     |[{404, 2.1045675}, {334, 2.0156727}, {296, 1.9900073}, {300, 1.9811398}, {360, 1.9764503}, {384, 1.9761128}, {283, 1.9755534}, {353, 1.974976}, {245, 1.9745234}, {374, 1.9645573}]|\n",
      "|34     |[{404, 1.8041916}, {360, 1.7999492}, {334, 1.7909286}, {375, 1.7745564}, {353, 1.7631757}, {391, 1.7532632}, {384, 1.7379528}, {296, 1.737398}, {245, 1.733621}, {283, 1.729852}]  |\n",
      "|53     |[{404, 1.9829658}, {334, 1.9108629}, {360, 1.8817106}, {296, 1.880996}, {353, 1.8737153}, {300, 1.8730516}, {384, 1.8702929}, {283, 1.8683498}, {245, 1.8680785}, {391, 1.8564993}]|\n",
      "|65     |[{404, 2.071029}, {334, 1.9866619}, {296, 1.9599723}, {300, 1.9511704}, {360, 1.9499937}, {384, 1.946864}, {353, 1.9466778}, {283, 1.945919}, {245, 1.9449538}, {374, 1.9343408}]  |\n",
      "|78     |[{404, 1.9068747}, {334, 1.8692049}, {360, 1.8625321}, {353, 1.837073}, {375, 1.8268809}, {296, 1.8244936}, {391, 1.8239326}, {384, 1.8203744}, {245, 1.8168187}, {300, 1.8162459}]|\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_recommendations = als_model.recommendForAllUsers(5)\n",
    "user_recommendations.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "def pretty_print_recommendations(user_id, recommendations, transactions_new):\n",
    "    \"\"\"\n",
    "    Pretty-print recommendations for a user.\n",
    "    \n",
    "    Args:\n",
    "    - user_id: The user_id to display recommendations for.\n",
    "    - recommendations: List of recommendations from ALS.\n",
    "    - transactions_new: DataFrame containing item metadata (item_id, name, category).\n",
    "    \"\"\"\n",
    "    # Get recommendations for the specified user\n",
    "    user_recs = next((row for row in recommendations if row.user_id == user_id), None)\n",
    "    if not user_recs:\n",
    "        print(f\"No recommendations found for user: {user_id}\")\n",
    "        return\n",
    "    \n",
    "    # Extract recommended item IDs\n",
    "    recommended_items = [row.item_id for row in user_recs.recommendations]\n",
    "\n",
    "    # Pretty print recommendations\n",
    "    print(f\"\\nShowing recommendations for user: {user_id}\")\n",
    "    print(\"=\" * 40)\n",
    "    print(recommended_items)\n",
    "    # final all recommended items from the transactions_new dataframe\n",
    "    recommended_items_df = transactions_new.filter(col(\"item_id\").isin(recommended_items))\n",
    "    recommended_items_df.show(truncate=False)\n",
    "    return recommended_items_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Showing recommendations for user: 10\n",
      "========================================\n",
      "[404, 334, 296, 360, 353, 300, 384, 283, 245, 391]\n",
      "+-------+--------------------------------------+-----------------+\n",
      "|item_id|name                                  |category         |\n",
      "+-------+--------------------------------------+-----------------+\n",
      "|245    |Google Men's Softshell Moss           |Apparel          |\n",
      "|283    |Google Cambridge Campus Bottle        |Campus Collection|\n",
      "|296    |Google LA Campus Lapel Pin            |Campus Collection|\n",
      "|300    |Google Men's Puff Jacket Black        |Apparel          |\n",
      "|360    |Google Seattle Campus Sticker         |Campus Collection|\n",
      "|384    |Google Tee Green                      |Clearance        |\n",
      "|391    |Google Mountain View Campus Ladies Tee|Campus Collection|\n",
      "|404    |Google Cork Pencil Pouch              |New              |\n",
      "+-------+--------------------------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[item_id: int, name: string, category: string]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example user_id\n",
    "test_user_id = 10\n",
    "\n",
    "# Pretty print recommendations for the user\n",
    "pretty_print_recommendations(test_user_id, user_recommendations_list, transactions_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
